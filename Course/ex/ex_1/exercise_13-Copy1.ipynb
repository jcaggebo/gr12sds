{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 13: Model building process and model selection\n",
    "\n",
    "*Morning, August 21, 2018*\n",
    "\n",
    "In this Exercise Set 13 we will investigate how to build machine learning models using a formalize pipeline from preprocessed (i.e. tidy) data to a model.\n",
    "\n",
    "We import our standard stuff. Notice that we are not interested in seeing the convergence warning in scikit-learn so we suppress them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "\n",
    "In what follows we will regard the \"train\" data for two purposes. First we are interested in performing a model selection. Then with the selected model we estimate/train it on all the training data. \n",
    "\n",
    "\n",
    "> **Ex. 13.1.0:** Begin by reloading the housing dataset from Ex. 12.2.0 using the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = pd.DataFrame(cal_house['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 13.1.1:** Make a for loop with 10 iterations where you:\n",
    "1. Split the input data into, train and test where the sample of test should be one third. (Set a new random state for each iteration of the loop, so each iteration makes a different split)\n",
    "2. Further split the training data  into to even sized bins; the first data is for training models and the other is for validation. \n",
    "3. Train a linear regression model with sub-training data. Compute the RMSE for out-of-sample predictions on test and validation data. Save the RMSE.\n",
    "\n",
    "> You should now have a 10x2 DataFrame with 10 RMSE from both the test data set and the train data set. Compute descriptive statistics of RMSE for the out-of-sample predictions on test and validation data. Are the simular?    \n",
    ">   They hopefuly are pretty simular. This shows us, that we can split the train data, and use this to fit the model. \n",
    "\n",
    ">> *Hint*: you can reuse any code used to solve exercises 12.2.X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.641, 0.601],\n",
       " [0.613, 0.619],\n",
       " [0.649, 0.591],\n",
       " [0.621, 0.602],\n",
       " [0.622, 0.623],\n",
       " [0.623, 0.595],\n",
       " [0.649, 0.601],\n",
       " [0.621, 0.588],\n",
       " [0.598, 0.608],\n",
       " [0.632, 0.605]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "n_iter = range(10)\n",
    "perform = []\n",
    "\n",
    "for i in n_iter:\n",
    "    # splitting into development (2/3) and test data (1/3)\n",
    "    X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=i)\n",
    "    \n",
    "    # splitting development into train (1/3) and validation (1/3)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=i)\n",
    "    \n",
    "    pipe_lr = make_pipeline(LinearRegression())\n",
    "\n",
    "    pipe_lr.fit(X_dev,y_dev)\n",
    "    \n",
    "    perform.append([round(mse(pipe_lr.predict(X_test),y_test), 3), round(mse(pipe_lr.predict(X_train),y_train), 3)])\n",
    "    \n",
    "perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 13.1.2:** Construct a model building pipeline which \n",
    "\n",
    "> 1. adds polynomial features without bias;\n",
    "> 1. scales the features to mean zero and unit std. \n",
    "> 1. estimates a Lasso model\n",
    "\n",
    ">> *Hint:* a modelling pipeline can be constructed with `make_pipeline` from `sklearn.pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.328, 1.337],\n",
       " [1.31, 1.343],\n",
       " [1.361, 1.317],\n",
       " [1.311, 1.323],\n",
       " [1.316, 1.363],\n",
       " [1.371, 1.29],\n",
       " [1.343, 1.309],\n",
       " [1.352, 1.326],\n",
       " [1.292, 1.348],\n",
       " [1.348, 1.343]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Answer to Ex. 13.1.2]\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "n_iter = range(10)\n",
    "perform = []\n",
    "\n",
    "for i in n_iter:\n",
    "    # splitting into development (2/3) and test data (1/3)\n",
    "    X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=i)\n",
    "\n",
    "    # splitting development into train (1/3) and validation (1/3)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=i)\n",
    "\n",
    "    pipe_lasso = make_pipeline(PolynomialFeatures(include_bias=False), \n",
    "                               StandardScaler(),\n",
    "                               Lasso(random_state=i))\n",
    "    pipe_lasso.fit(X_dev,y_dev)\n",
    "    perform.append([round(mse(pipe_lasso.predict(X_test),y_test), 3), round(mse(pipe_lasso.predict(X_train),y_train), 3)])\n",
    "\n",
    "perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "In machine learning, we have two types of parameters: those that are learned from\n",
    "the training data, for example, the weights in logistic regression, and the parameters\n",
    "of a learning algorithm that are optimized separately. The latter are the tuning\n",
    "parameters, also called *hyperparameters*, of a model, for example, the regularization\n",
    "parameter in logistic regression or the depth parameter of a decision tree.\n",
    "  \n",
    "   \n",
    "When we want to optimize over both normal parameters and hyperparameteres we do this using nested loops (two-layered cross validation). In outer loop we vary the hyperparameters, and then in the inner loop we do cross validation for the model with the specific selection of hyperparameters. This way we can find the model, with the lowest mean MSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 13.1.3:**\n",
    "Run a Lasso regression using the Pipeline from `Ex 13.1.2`. In the outer loop searching through the lambdas specified below. \n",
    "In the inner loop make 5 fold cross validation on the selected model and store the *average* MSE for each fold. Which lambda gives the lowest test MSE?\n",
    "\n",
    "\n",
    "> ```python \n",
    "lambdas =  np.logspace(-4, 4, 12)\n",
    "```\n",
    "\n",
    ">> *Hint:* `KFold` in `sklearn.model_selection` may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "kfolds = KFold(n_splits=5)\n",
    "mseCV = []\n",
    "\n",
    "lambdas = np.logspace(-4,4,12)\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=1)\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    pipe_lassoCV = make_pipeline(PolynomialFeatures(include_bias=False), \n",
    "                               StandardScaler(),\n",
    "                               Lasso(alpha=lambda_, random_state=1))\n",
    "     \n",
    "    mseCV_ = []\n",
    "    \n",
    "    for train_idx, val_idx in kfolds.split(X_dev, y_dev):\n",
    "        \n",
    "        X_train, y_train = X_dev.iloc[train_idx], y_dev.iloc[train_idx]\n",
    "        X_val, y_val = X_dev.iloc[val_idx], y_dev.iloc[val_idx] \n",
    "\n",
    "        pipe_lassoCV.fit(X_train, y_train)\n",
    "        \n",
    "        mseCV_.append(mse(pipe_lassoCV.predict(X_val), y_val))    \n",
    "    mseCV.append(mseCV_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015199    0.651984\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimalCV = pd.DataFrame(mseCV, index=lambdas).mean(axis=1).nsmallest(1)\n",
    "optimalCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 13.1.4:** *Automated Cross Validation in one dimension:* \n",
    "Now we want to repeat exercise 13.1.3 in a more automated fasion. \n",
    "When you are doing cross validation with one hyperparameter, you can automate the process by using `validation_curve` from `sklearn.model_selection`. Use this function to search through the values of lambda, and find the value of lambda, which give the lowest test error.  \n",
    "\n",
    "> check if you got the same output for the manual implementation (Ex. 13.1.3) and the automated implementation (Ex. 13.1.4) \n",
    "\n",
    "> *Hint:* You should set the scoring parameter to `neg_mean_squared_error`\n",
    "\n",
    "> BONUS: Plot the average MSE-test and MSE-train against the different values of lambda. (*Hint*: Use logarithmic axes, and lambda as index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda\n",
      "0.015199    0.620941\n",
      "Name: Test, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "train_scores, test_scores = \\\n",
    "    validation_curve(estimator=pipe_lasso,\n",
    "                     X=X_train,\n",
    "                     y=y_train,\n",
    "                     param_name='lasso__alpha',\n",
    "                     param_range=lambdas,\n",
    "                     scoring='neg_mean_squared_error',                 \n",
    "                     cv=10)\n",
    "mse_score = pd.DataFrame({'Train':-train_scores.mean(axis=1),\n",
    "                          'Test':-test_scores.mean(axis=1),\n",
    "                          'lambda':lambdas})\\\n",
    "              .set_index('lambda')   \n",
    "print(mse_score.Test.nsmallest(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a24de9f28>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse_score.plot(logx=True, logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have *more that one* hyperparameter, you will want to fit the model to all the possible combinations of hyperparameters. This is done in an approch called `Grid Search`, which is implementet in `sklearn.model_selection` as `GridSearchCV`\n",
    "\n",
    "> **Ex. 13.1.5:** To get to know `Grid Search` we want to implement it in one dimension. Using `GridSearchCV` implement the Lasso, with the same lambdas as before (`lambdas =  np.logspace(-4, 4, 12)`), 10-fold CV and (negative) mean squared error as the scoring variable. Which value of Lambda gives the lowest test error? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lasso__alpha': 0.01519911082952933}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "%timeit -n1\n",
    "gs = GridSearchCV(estimator=pipe_lasso, \n",
    "                  param_grid=[{'lasso__alpha':lambdas}], \n",
    "                  scoring='neg_mean_squared_error', \n",
    "                  cv=10, \n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 13.1.6 BONUS** Expand the Lasso pipe from the last excercise with a Principal Component Analisys (PCA), and expand the Grid Search to searching in two dimensions (both along the values of lambda and the values of principal components (n_components)). Is `n_components` a hyperparameter? Which hyperparameters does the Grid Search select as the best?\n",
    "\n",
    "> NB. This might take a while to calculate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 13.1.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 13.1.7 BONUS** repeat the previous now with RandomizedSearchCV with 20 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 13.1.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Ex. 13.1.8 BONUS** read about nested cross validation. How might we implement this in answer 13.1.6?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 13.1.8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
