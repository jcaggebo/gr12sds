{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpCk3_2VLXLl"
   },
   "source": [
    "# Documentation for exam paper in Social Data Science 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rj0QRuEaQ_29"
   },
   "source": [
    "## Libraries\n",
    "Here is all the libraries that we use in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmZzzN7-Q_2-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import tqdm\n",
    "import re\n",
    "import nltk\n",
    "from numpy import NaN\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from nltk import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.parsing.preprocessing\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "from multiprocessing import Pool\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EjCBiKnHQ_3A"
   },
   "source": [
    "## Web scraping\n",
    "\n",
    "The code block below contains all functions for scrabing lyrics from genius.com.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vd-YRcczQ_3B"
   },
   "outputs": [],
   "source": [
    "# function that generates n pages for a given tag\n",
    "def page_urls(n, tag):\n",
    "  \n",
    "  # generates the base url for a given tag\n",
    "    base_url = 'https://genius.com/tags/' + tag + '/all?page='\n",
    "\n",
    "    # define list for storing page links\n",
    "    pages = []\n",
    "\n",
    "    # for loop that generates n page links\n",
    "    for i in range(n):\n",
    "        page = base_url + str(i)\n",
    "        pages.append(page)\n",
    "    return pages\n",
    "\n",
    "# function that gets all links from a given page\n",
    "def get_links(html):\n",
    "\n",
    "    # locating page results which contains list with lyrics\n",
    "    soup = BeautifulSoup(html) \n",
    "    page_results = str(soup.find('ul',attrs={'class':'search_results song_list primary_list'}))\n",
    "\n",
    "    # define set for storing links\n",
    "    links = set()\n",
    "\n",
    "    # for loop that locates the beginning of all links from the page result\n",
    "    for link_loc in page_results.split('href=\"')[1:]:\n",
    "    \n",
    "        # splitting the link\n",
    "        link = link_loc.split('\"')[0]\n",
    "      \n",
    "        # adds the link to the set\n",
    "        links.add(link)\n",
    "    return links\n",
    "\n",
    "# rate limit function that sleeps one second\n",
    "def ratelimit():\n",
    "    time.sleep(1)\n",
    "    \n",
    "# reliable requests\n",
    "def get(url,iterations=10,check=lambda x: x.ok):\n",
    "    # requests the url 10 times if response is false\n",
    "    for iteration in range(iterations):\n",
    "        try:\n",
    "            ratelimit()\n",
    "            response = requests.get(url)\n",
    "            if check(response):\n",
    "                return response\n",
    "        except requests.exceptions as e:\n",
    "            print(e)\n",
    "    return None\n",
    "\n",
    "# function that gets the lyrics in text format from link\n",
    "def get_lyrics(html):\n",
    "    # converts html from link to a soup object\n",
    "    soup = BeautifulSoup(html, \"html.parser\") \n",
    "    # gets the lyrics in text format\n",
    "    lyrics = soup.find(\"div\", class_=\"lyrics\").get_text()\n",
    "    lyrics_dict = {'lyrics':lyrics}\n",
    "    return lyrics_dict\n",
    "\n",
    "# function that merges two dicts\n",
    "def combine_dicts(x, y):\n",
    "    # makes copy of dict x in variable z\n",
    "    z = x.copy()\n",
    "    # update dict z with dict y - merging\n",
    "    z.update(y)\n",
    "    # return dict z\n",
    "    return z\n",
    "\n",
    "# function that extracts the release date of the song\n",
    "def get_year(html):\n",
    "    # splitting after release date text in the html\n",
    "    year1 = html.split('&quot;release_date&quot;')[1:]\n",
    "    # splitting such that only the date is stored\n",
    "    year2 = year1[0].split('&quot;')[1]\n",
    "    # creating dictionary with the year\n",
    "    year_json = {'year':year2}\n",
    "    return year_json\n",
    "\n",
    "# function song meta data\n",
    "def get_details(html):\n",
    "    # splitting after tracing_data text in the html\n",
    "    data = html.split('TRACKING_DATA =')[1:]\n",
    "    # splitting such that all the tracking data is stored\n",
    "    data2 = data[0].split('}')[0] +'}'\n",
    "    # creating dictionary with the meta data\n",
    "    data3 = json.loads(str(data2).strip())\n",
    "    # getting release date\n",
    "    year = get_year(html)\n",
    "    # merging release date on meta data\n",
    "    details = combine_dicts(data3,year)\n",
    "    # returning final dict with meta data\n",
    "    return details\n",
    "\n",
    "# function that can filter dictionaries\n",
    "dictfilt = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])\n",
    "\n",
    "# function that scrapes all the wanted data and returns it as a dictionary\n",
    "def get_final(link):\n",
    "    # if the link does not respond, log the link as False\n",
    "    if get(link) == None:\n",
    "        with open('log.txt', 'a') as f:\n",
    "            f.write('[' + str(link) + ',' + str(False) + '],')\n",
    "        # qutting such that the program doesnt stop due to non responding links\n",
    "        return None\n",
    "  # if the link does respond, log the link as True, and proceed\n",
    "    else:\n",
    "        html = get(link).text\n",
    "        lyrics = get_lyrics(html)\n",
    "        details = get_details(html)\n",
    "        keep = set(['Annotatable Type', 'Lyrics Language', 'Primary Artist', 'Primary Tag', 'Song ID', 'Title', 'lyrics', 'year'])\n",
    "        combined = combine_dicts(lyrics,details)\n",
    "        with open('log.txt', 'a') as f:\n",
    "            f.write('[' + str(link) + ',' + str(True) + '],')\n",
    "    return dictfilt(combined,keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wPh8KOl9Q_3D"
   },
   "source": [
    "We now generate alle the page urls, that are used to scrape the individual song lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_S3fBD3Q_3E"
   },
   "outputs": [],
   "source": [
    "# page urls for each genre\n",
    "pages_rock = page_urls(51, 'rock')\n",
    "pages_rap = page_urls(51, 'rap')\n",
    "pages_country = page_urls(51, 'country')\n",
    "pages_pop = page_urls(51, 'pop')\n",
    "\n",
    "# combining all page lists\n",
    "pages = pages_rock + pages_rap + pages_country + pages_pop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVwmQ_rRQ_3F"
   },
   "source": [
    "The scraper now iterates through all page urls to collect all song urls. The song urls is collected in a set to avoid duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "shL-XU8bQ_3G"
   },
   "outputs": [],
   "source": [
    "song_links = set() # container that gets rid og duplicates\n",
    "for song in tqdm.tqdm(pages): # set len(pages) to scrape all songs from page links\n",
    "    html = get(song).text # getting the html\n",
    "    links = get_links(html) # extracting links\n",
    "    for link in links:\n",
    "        song_links.add(link)\n",
    "song_links = list(song_links)\n",
    "df = pd.DataFrame(song_links)\n",
    "df.to_csv('links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nq3LtKLTQ_3I"
   },
   "source": [
    "The scraper now iterates over alle song urls to extract the need information from the html code. Finally, the list is converted to pandas dataframe and saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mzv_8290Q_3J"
   },
   "outputs": [],
   "source": [
    "song_links = pd.read_csv('links.csv')\n",
    "print('There is ' + str(len(song_links)) + ' unique song links.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "haEr4P9vQ_3L"
   },
   "outputs": [],
   "source": [
    "lyrics = []\n",
    "for i in tqdm.tqdm(song_links):\n",
    "    lyric = get_final(i)\n",
    "    lyrics.append(lyric)\n",
    "\n",
    "df_lyrics = pd.DataFrame(lyrics)\n",
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JSVCnlKoQ_3M"
   },
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZEG5HgfEUHgZ"
   },
   "source": [
    "We load in the data generated from the web scraping process above. We clean, select and preprocess the data. The size of the final data set is reported, when running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8g9KW7dQ_3M",
    "outputId": "45b1cc16-effd-41bb-8abc-ff2262c1c24d"
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "print('The loaded data set contains of ' + str(len(df)) + ' lyrics')\n",
    "\n",
    "# fitting columns\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "cols = ['type', 'language', 'artist', 'tag', 'id', 'title', 'lyrics', 'release_date']\n",
    "df.columns = cols\n",
    "print('The column names of the data set is ' + str(cols))\n",
    "\n",
    "# slicing the release date to extract year\n",
    "df['year'] = df['release_date'].apply(lambda i: i[:4]).apply(lambda i: re.sub('rele', '0', i)).apply(lambda i:int(i))\n",
    "\n",
    "# selection criteria\n",
    "select_en = df.language == 'en' # select only english text\n",
    "select_tag = df.tag != 'non-music' # select only music\n",
    "select_cat = df.tag != 'r-b' # select only rock, pop, rap, country\n",
    "select_rows = select_en & select_tag & select_cat # combining criteria\n",
    "\n",
    "# applying criteria\n",
    "df_select = df[select_rows == True].reset_index().copy()\n",
    "lost = len(df)-len(df_select)\n",
    "print('We loose ' + str(lost) + ' lyrics by applying the selection criteria, the data set now contain ' + str(len(df_select)))\n",
    "\n",
    "# lower case and punctuation clean\n",
    "df_select['lyrics'] = df_select.loc[:,'lyrics'].apply(lambda i: re.sub('\\[[^\\]]+\\]', '', i)) # removes chorus\n",
    "df_select['lyrics_clean'] = df_select.lyrics.apply(lambda i: i.lower()) # converts to lower case\n",
    "df_select['lyrics_clean'] = df_select.lyrics_clean.apply(lambda i: re.sub('[^0-9a-z\\']+', ' ', i)) #removes non characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3P_zJy8RWoV"
   },
   "source": [
    "### Tokenizing of the lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NM3o2Mq1U2wY"
   },
   "source": [
    "We tokenize the words using tweet tokenizer, since this package is better at handling words with appostrphes. There after stop words is removed. Finally, we also create a token list exxcluding some specially selected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-23ZLD5qQ_3P"
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "\n",
    "df_select['word_tokens'] = df_select['lyrics_clean'].apply(lambda i: tknzr.tokenize(i))\n",
    "\n",
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "df_select['word_tokens2'] = df_select['word_tokens'].apply(lambda i: [j for j in i if j not in stopWords])\n",
    "\n",
    "# remove stop words including sound words\n",
    "new_stopwords = set([\"oh\", \"ooh\", \"yeah\", \"na\", \"la\", \"uh\", \"ayy\", \"da\", \"hey\", \"yo\", \"yay\", \"ah\", \"aya\",\"\\'\"])\n",
    "df_select['word_tokens3'] = df_select['word_tokens2'].apply(lambda i: [j for j in i if j not in new_stopwords])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObRhi_h_VF4h"
   },
   "source": [
    "We now create bigrams and trigrams that are used for our topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9ukVLFYQ_3R"
   },
   "outputs": [],
   "source": [
    "# bigrams and trigrams\n",
    "bigram = gensim.models.Phrases(df_select['word_tokens3'], min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[df_select['word_tokens3']], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "df_select['bigrams'] = make_bigrams(df_select['word_tokens3'])\n",
    "df_select['word_tokens4'] = make_trigrams(df_select['bigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pML3YDeQ_3U"
   },
   "outputs": [],
   "source": [
    "# stemming words\n",
    "ps = PorterStemmer()\n",
    "df_select['word_tokens5'] = df_select['word_tokens4'].apply(lambda i: [ps.stem(j) for j in i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8ZmhXr7Q_3V"
   },
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLmVbEaMQ_3V"
   },
   "source": [
    "The following code contains a plot for the distribution of songs across genres and years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFk01FBWQ_3V",
    "outputId": "4383bd7e-e4e0-432b-bc63-292de5aff0f2"
   },
   "outputs": [],
   "source": [
    "# font settings\n",
    "font = {'family' : 'arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# setting plot settings\n",
    "fig, axes = plt.subplots(ncols=2, figsize = (10,5))\n",
    "ax1, ax2 = axes.flatten()\n",
    "\n",
    "# plot input for subplot 1\n",
    "data = dict(df_select['tag'].value_counts())\n",
    "tags = list(data.keys())\n",
    "count = list(data.values())\n",
    "\n",
    "# setting title\n",
    "ax1.set_title('(a)\\nNumber of songs', pad = 20)\n",
    "\n",
    "# removes plot frame\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "ax1.spines['left'].set_visible(False)\n",
    "\n",
    "\n",
    "# adjusts x-axis and y-axis\n",
    "# removing y-axis\n",
    "ax1.set_yticks([])\n",
    "\n",
    "# removes y-ticks and x-ticks\n",
    "ax1.tick_params(axis='both', which='both', length=0) \n",
    "\n",
    "# setting x-ticks\n",
    "plt.xticks([0,1,2,3], ('Rap', 'Pop', 'Country', 'Rock'))\n",
    "\n",
    "# makes bar plot\n",
    "b0, b1, b2, b3 = ax1.bar(('Rap', 'Pop', 'Country', 'Rock'), count) # barplot input\n",
    "b0.set_facecolor('#004c6d') # color for bar 1\n",
    "b1.set_facecolor('#3d708f') # color for bar 2\n",
    "b2.set_facecolor('#6996b3') # color for bar 3\n",
    "b3.set_facecolor('#94bed9') # color for bar 4\n",
    "\n",
    "# setting text attributes to bar plot, loops over all bars\n",
    "for bar in [b0,b1,b2,b3]: \n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2.0, height, '%d' % int(height), ha='center', va='bottom')\n",
    "    \n",
    "# plot input for subplot 2\n",
    "year_dist = df_select[(df_select.year != 0) == True].year\n",
    "x1 = sum(i >= 2015 for i in year_dist)\n",
    "x2 = sum(i >= 2010 for i in year_dist) - x1\n",
    "x3 = sum(i >= 1990 for i in year_dist) - x1 - x2\n",
    "x4 = sum(i >= 1970 for i in year_dist) - x1 - x2 - x3\n",
    "x5 = sum(i <= 1969 for i in year_dist)\n",
    "labels = ['1850-1969','1970-1989','1990-2009','2010-2014', '2015-2018']\n",
    "count1 = [x5, x4, x3, x2, x1]\n",
    "\n",
    "# setting title\n",
    "ax2.set_title('(b)\\nNumber of songs in year intervals', pad = 20)\n",
    "\n",
    "# removes plot frame\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['bottom'].set_visible(False)\n",
    "ax2.spines['left'].set_visible(False)\n",
    "\n",
    "\n",
    "# adjusts x-axis and y-axis\n",
    "# removing y-axis\n",
    "ax2.set_yticks([]) \n",
    "\n",
    "# removing y-ticks and x-ticks\n",
    "ax2.tick_params(axis='both', which='both', length=0) \n",
    "\n",
    "# Over writing x-ticks\n",
    "plt.xticks([0,1,2,3,4], ('1850-1969','1970-1989','1990-2009','2010-2014', '2015-2018'), fontsize = 11) \n",
    "\n",
    "# makes bar plot\n",
    "b0_uw, b1_uw, b2_uw, b3_uw, b4_uw = ax2.bar([0,1,2,3,4], count1) # barplot input\n",
    "b0_uw.set_facecolor('#004c6d') # color for bar 1\n",
    "b1_uw.set_facecolor('#3d708f') # color for bar 2\n",
    "b2_uw.set_facecolor('#6996b3') # color for bar 3\n",
    "b3_uw.set_facecolor('#94bed9') # color for bar 4\n",
    "b4_uw.set_facecolor('#c1e7ff') # color for bar 5\n",
    "\n",
    "# setting text attributes to bar plot, loops over all bars\n",
    "for bar in [b0_uw,b1_uw,b2_uw,b3_uw,b4_uw]: \n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, height, '%d' % int(height), ha='center', va='bottom')\n",
    "    \n",
    "# displaying and saving figure\n",
    "fig.tight_layout()\n",
    "plt.show(ax1, ax2)\n",
    "fig.savefig('dist_plot.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fDa1AAB7Q_3X",
    "outputId": "4607e93f-d49a-4921-92a5-4c62d42ca969"
   },
   "outputs": [],
   "source": [
    "print('The number of unique artists is ' + str(df_select['artist'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYUWsyTDQ_3Y",
    "outputId": "d76d3adf-ad47-4bcf-b4c8-57b808540d3c"
   },
   "outputs": [],
   "source": [
    "df_select.groupby('tag')['artist'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NeErWbFcQ_3a"
   },
   "source": [
    "## Descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZU227sqQ_3a"
   },
   "source": [
    "Creating new variables for characterising the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OVyC562UQ_3a"
   },
   "outputs": [],
   "source": [
    "# unique words\n",
    "df_select['unique_words'] = df_select['word_tokens'].apply(lambda i: pd.DataFrame(i).nunique())\n",
    "# number of words\n",
    "df_select['word_count'] = df_select.word_tokens.apply(lambda i: len(i))\n",
    "# share of unique words\n",
    "df_select['share_unique_words'] = df_select['unique_words']/df_select['word_count']*100\n",
    "# characters in lyric\n",
    "df_select['characters'] = df_select.lyrics_clean.apply(lambda i: len(re.sub(' ', '',i)))\n",
    "# characters per word\n",
    "df_select['characters_per_word'] = df_select.characters/df_select.word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3qMLYIglQ_3b",
    "outputId": "e41ab71c-3d35-41fc-8416-71de86644036"
   },
   "outputs": [],
   "source": [
    "df_select.groupby('tag')['share_unique_words'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOQPWXohQ_3d",
    "outputId": "982de754-d885-457c-faf3-de004d016740"
   },
   "outputs": [],
   "source": [
    "df_select.groupby('tag')['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHdazs_8Q_3e",
    "outputId": "1354db2c-23f3-42d8-fdd1-51ddced778ff"
   },
   "outputs": [],
   "source": [
    "df_select.groupby('tag')['characters_per_word'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5BA6B6cR08w"
   },
   "source": [
    "The following code block generates a plot for the average word count and the average share of unique words across the four genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3lsl4VCQ_3g",
    "outputId": "251b692d-3261-4fc7-92f9-1371c16c2d9d"
   },
   "outputs": [],
   "source": [
    "# font settings\n",
    "font = {'family' : 'arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# setting plot settings\n",
    "fig, axes = plt.subplots(ncols=2, figsize = (10,5))\n",
    "ax1, ax2 = axes.flatten()\n",
    "\n",
    "\n",
    "# plot input for subplot 1\n",
    "mean = df_select.groupby(['tag'])['word_count'].mean()\n",
    "mean_df = pd.DataFrame(mean).sort_values(by=['word_count'], ascending=False)\n",
    "count = list(mean_df['word_count'])\n",
    "\n",
    "# setting title\n",
    "ax1.set_title('(a)\\nAverage number of words', pad = 20)\n",
    "\n",
    "# removes plot frame\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "ax1.spines['left'].set_visible(False)\n",
    "\n",
    "\n",
    "# adjusts x-axis and y-axis\n",
    "# removing y-axis\n",
    "ax1.set_yticks([])\n",
    "\n",
    "# removes y-ticks and x-ticks\n",
    "ax1.tick_params(axis='both', which='both', length=0) \n",
    "\n",
    "# setting x-ticks\n",
    "plt.xticks([0,1,2,3], ('Rap', 'Pop', 'Country', 'Rock'))\n",
    "\n",
    "\n",
    "\n",
    "# makes bar plot\n",
    "b0, b1, b2, b3 = ax1.bar(('Rap', 'Pop', 'Country', 'Rock'), count) # barplot input\n",
    "b0.set_facecolor('#004c6d') # color for bar 1\n",
    "b1.set_facecolor('#3d708f') # color for bar 2\n",
    "b2.set_facecolor('#6996b3') # color for bar 3\n",
    "b3.set_facecolor('#94bed9') # color for bar 4\n",
    "\n",
    "\n",
    "# setting text attributes to bar plot, loops over all bars\n",
    "for bar in [b0,b1,b2,b3]: \n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2.0, height, '%d' % int(height), ha='center', va='bottom')\n",
    "\n",
    "    \n",
    "# plot input for subplot 2\n",
    "mean_uw = df_select.groupby(['tag'])['share_unique_words'].mean()\n",
    "mean_uw_sort = pd.DataFrame(mean_uw).sort_values(by=['share_unique_words'], ascending=False)\n",
    "count1 = list(mean_uw_sort['share_unique_words'])\n",
    "\n",
    "# setting title\n",
    "ax2.set_title('(b)\\nShare of unique words', pad = 20)\n",
    "\n",
    "# removes plot frame\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['bottom'].set_visible(False)\n",
    "ax2.spines['left'].set_visible(False)\n",
    "\n",
    "\n",
    "# adjusts x-axis and y-axis\n",
    "# removing y-axis\n",
    "ax2.set_yticks([]) \n",
    "\n",
    "# removing y-ticks and x-ticks\n",
    "ax2.tick_params(axis='both', which='both', length=0) \n",
    "\n",
    "# Over writing x-ticks\n",
    "plt.xticks([0,1,2,3], ('Country', 'Rock', 'Rap', 'Pop')) # Over writing x-ticks\n",
    "\n",
    "# makes bar plot\n",
    "b0_uw, b1_uw, b2_uw, b3_uw = ax2.bar([0,1,2,3], count1) # barplot input\n",
    "b0_uw.set_facecolor('#004c6d') # color for bar 1\n",
    "b1_uw.set_facecolor('#3d708f') # color for bar 2\n",
    "b2_uw.set_facecolor('#6996b3') # color for bar 3\n",
    "b3_uw.set_facecolor('#94bed9') # color for bar 4\n",
    "\n",
    "# setting text attributes to bar plot, loops over all bars\n",
    "for bar in [b0_uw,b1_uw,b2_uw,b3_uw]: \n",
    "    height = bar.get_height()\n",
    "    heighttxt = str(round(height,1)) + ' pct.'\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, height, heighttxt, ha='center', va='bottom')\n",
    "    \n",
    "# displaying and saving figure\n",
    "fig.tight_layout()\n",
    "plt.show(ax1, ax2)\n",
    "fig.savefig('word_plot.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDGj9U2HQ_3h"
   },
   "source": [
    "Boxplot displaying the distribution of the average characters per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kaox9flaQ_3h",
    "outputId": "1443898d-ec23-43ee-f8bf-ba469be00f1a"
   },
   "outputs": [],
   "source": [
    "# font settings\n",
    "font = {'family' : 'arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# figure settings\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# data for the boxplot\n",
    "data_to_plot = df_select[['characters_per_word','tag']]\n",
    "data_rap =np.array(data_to_plot[(data_to_plot.tag == 'rap') == True]['characters_per_word'])\n",
    "data_rock = np.array(data_to_plot[(data_to_plot.tag == 'pop') == True]['characters_per_word'])\n",
    "data_pop = np.array(data_to_plot[(data_to_plot.tag == 'rock') == True]['characters_per_word'])\n",
    "data_country = np.array(data_to_plot[(data_to_plot.tag == 'country') == True]['characters_per_word'])\n",
    "data_box = [data_rap,data_rock,data_pop,data_country]\n",
    "\n",
    "# removes frame\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xticklabels(['Rap', 'Rock', 'Pop', 'Country'])\n",
    "\n",
    "# enables adding colors\n",
    "bp = ax.boxplot(data_box, patch_artist=True)\n",
    "\n",
    "# change outline color, fill color and linewidth of the boxes\n",
    "\n",
    "# looping over all boxes to change the line color and fill\n",
    "for box in bp['boxes']:\n",
    "    box.set( color='#3d708f', linewidth=2)\n",
    "    box.set( facecolor = '#3d708f' )\n",
    "\n",
    "# lopping over all whiskers to change color\n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set(color='#004c6d', linewidth=2)\n",
    "\n",
    "# lopping over all caps to change color\n",
    "for cap in bp['caps']:\n",
    "    cap.set(color='#004c6d', linewidth=2)\n",
    "\n",
    "# lopping over all medians to change color\n",
    "for median in bp['medians']:\n",
    "    median.set(color='#94bed9', linewidth=2)\n",
    "\n",
    "# lopping over all outlier marks to change color and apperance\n",
    "for flier in bp['fliers']:\n",
    "    flier.set(marker='o', color='#004c6d', alpha=0.2)\n",
    "\n",
    "# setting xticklabels\n",
    "ax.set_xticklabels(['Rap', 'Pop', 'Rock', 'Country'])\n",
    "\n",
    "# setting title\n",
    "ax.set_title('Characters per word', pad = 20)\n",
    "\n",
    "# removing y-ticks and x-ticks\n",
    "ax.tick_params(axis='both', which='both', length=0) \n",
    "\n",
    "# saving figure\n",
    "fig.savefig('box.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PHOkdyD-Q_3n"
   },
   "source": [
    "## Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5RKlbnlWQ_3n"
   },
   "outputs": [],
   "source": [
    "# function to generate wordcloud image\n",
    "def plot_word_cloud(words, filename=None):\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=600,\n",
    "        max_words=500,\n",
    "        scale=3,\n",
    "        background_color = 'white'\n",
    "    )\n",
    "    wordcloud.generate_from_frequencies(dict(words))\n",
    "    plt.figure(figsize=(15,20))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches='tight', dpi=500)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzcDo4uIQ_3o"
   },
   "outputs": [],
   "source": [
    "#Generating datasets for different genres\n",
    "select_rap = df_select.tag == 'rap'\n",
    "df_rap = df_select[select_rap==True]\n",
    "\n",
    "select_pop = df_select.tag == 'pop'\n",
    "df_pop = df_select[select_pop==True]\n",
    "\n",
    "select_rock = df_select.tag == 'rock'\n",
    "df_rock = df_select[select_rock==True]\n",
    "\n",
    "select_country = df_select.tag == 'country'\n",
    "df_country = df_select[select_country==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8h2SZngUQ_3p"
   },
   "source": [
    "### Rap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDK2z40CQ_3p",
    "outputId": "e38ea8e7-361b-4ade-e83c-16e88c19a7a4"
   },
   "outputs": [],
   "source": [
    "#Wordcloud for rap\n",
    "cv = CountVectorizer(max_features = 50, stop_words=stopWords, analyzer='word')\n",
    "counts_rap = cv.fit_transform(df_rap['lyrics_clean'])                                               \n",
    "words_rap = np.array(cv.get_feature_names()) \n",
    "\n",
    "number_rap = pd.DataFrame(counts_rap.A, columns=cv.get_feature_names())\n",
    "\n",
    "comeon_rap = number_rap.sum(axis=0)\n",
    "jeg_tror_den_er_her_rap=comeon_rap.to_dict()\n",
    "\n",
    "plot_word_cloud(jeg_tror_den_er_her_rap, filename = \"wordcloud_rap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Me6_AeUsQ_3q"
   },
   "source": [
    "### Pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zeITRK8-Q_3r",
    "outputId": "94047dca-2ac4-4191-ae20-6c33993cccf3"
   },
   "outputs": [],
   "source": [
    "#Wordcloud for pop\n",
    "cv = CountVectorizer(max_features = 50, stop_words=stopWords, analyzer='word')\n",
    "counts_pop = cv.fit_transform(df_pop['lyrics_clean'])                                               \n",
    "words_pop = np.array(cv.get_feature_names()) \n",
    "\n",
    "number_pop = pd.DataFrame(counts_pop.A, columns=cv.get_feature_names())\n",
    "\n",
    "comeon_pop = number_pop.sum(axis=0)\n",
    "jeg_tror_den_er_her_pop=comeon_pop.to_dict()\n",
    "\n",
    "plot_word_cloud(jeg_tror_den_er_her_pop, filename = \"wordcloud_pop.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdzzlL1_Q_3s"
   },
   "source": [
    "### Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0b23OR-rQ_3s",
    "outputId": "4473e1cf-8486-413e-b732-1c0265213942"
   },
   "outputs": [],
   "source": [
    "#Wordcloud for rock\n",
    "cv = CountVectorizer(max_features = 50, stop_words=stopWords, analyzer='word')\n",
    "counts_rock = cv.fit_transform(df_rock['lyrics_clean'])                                               \n",
    "words_rock = np.array(cv.get_feature_names()) \n",
    "\n",
    "number_rock = pd.DataFrame(counts_rock.A, columns=cv.get_feature_names())\n",
    "\n",
    "comeon_rock = number_rock.sum(axis=0)\n",
    "jeg_tror_den_er_her_rock=comeon_rock.to_dict()\n",
    "\n",
    "plot_word_cloud(jeg_tror_den_er_her_rock, filename = \"wordcloud_rock.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EW2kaM3TQ_3t"
   },
   "source": [
    "### Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNcpV08RQ_3t",
    "outputId": "89ad0c79-402b-4f9f-8811-7fd388365f74"
   },
   "outputs": [],
   "source": [
    "#Wordcloud for country\n",
    "cv = CountVectorizer(max_features = 50, stop_words=stopWords, analyzer='word')\n",
    "counts_country = cv.fit_transform(df_country['lyrics_clean'])                                               \n",
    "words_country = np.array(cv.get_feature_names()) \n",
    "\n",
    "number_country = pd.DataFrame(counts_country.A, columns=cv.get_feature_names())\n",
    "\n",
    "comeon_country = number_country.sum(axis=0)\n",
    "jeg_tror_den_er_her_country=comeon_country.to_dict()\n",
    "\n",
    "plot_word_cloud(jeg_tror_den_er_her_country, filename = \"wordcloud_country.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8wgByJVQ_3u"
   },
   "source": [
    "## Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T24OC5QsQ_3v"
   },
   "source": [
    "### Rap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFsnYoGeQ_3v"
   },
   "outputs": [],
   "source": [
    "processed_docs_rap = df_rap['word_tokens4']\n",
    "\n",
    "# Topic model with gensim\n",
    "\n",
    "dictionary_rap = gensim.corpora.Dictionary(processed_docs_rap)\n",
    "dictionary_rap.filter_extremes(no_below=15, no_above=0.5, keep_n = 10000)\n",
    "bow_corpus_rap = [dictionary_rap.doc2bow(doc) for doc in processed_docs_rap]\n",
    "n_cores = 12\n",
    "k = 2\n",
    "lda_model_rap = gensim.models.LdaMulticore(bow_corpus_rap, num_topics=k, id2word=dictionary_rap, passes=2, random_state=42)\n",
    "\n",
    "#lda_model_rap.save('rap.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Bos9vM6Q_3w",
    "outputId": "57487eca-4925-4ad2-deb5-ba3d1b64a088"
   },
   "outputs": [],
   "source": [
    "topics_rap = lda_model_rap.print_topics(num_words=10)\n",
    "for topic in topics_rap:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQndTtL9Q_3x",
    "outputId": "4dd2ee46-9948-4e84-be80-21405bdad6d1"
   },
   "outputs": [],
   "source": [
    "# Coherence Score\n",
    "coherence_model_lda_rap = CoherenceModel(model=lda_model_rap, texts=processed_docs_rap, dictionary=dictionary_rap, coherence='c_v')\n",
    "coherence_lda_rap = coherence_model_lda_rap.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_rap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dR79rJJlQ_3y"
   },
   "source": [
    "### Pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0Zrnj8HQ_3y"
   },
   "outputs": [],
   "source": [
    "processed_docs_pop = df_pop['word_tokens4']\n",
    "\n",
    "# Topic model with gensim\n",
    "\n",
    "dictionary_pop = gensim.corpora.Dictionary(processed_docs_pop)\n",
    "dictionary_pop.filter_extremes(no_below=15, no_above=0.6, keep_n = 10000)\n",
    "bow_corpus_pop = [dictionary_pop.doc2bow(doc) for doc in processed_docs_pop]\n",
    "n_cores = 12\n",
    "k = 2\n",
    "lda_model_pop = gensim.models.LdaMulticore(bow_corpus_pop, num_topics=k, id2word=dictionary_pop, passes=2, random_state = 42)\n",
    "\n",
    "#lda_model_pop.save('pop.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDceolppQ_3z",
    "outputId": "39dc0a5f-1f09-48d6-be6b-fbf2653a19ac"
   },
   "outputs": [],
   "source": [
    "topics_pop = lda_model_pop.print_topics(num_words=10)\n",
    "for topic in topics_pop:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RE-fWlMMQ_30",
    "outputId": "42f0f882-ddd9-4431-9f6f-97554b28f6ab"
   },
   "outputs": [],
   "source": [
    "# Coherence Score\n",
    "coherence_model_lda_pop = CoherenceModel(model=lda_model_pop, texts=processed_docs_pop, dictionary=dictionary_pop, coherence='c_v')\n",
    "coherence_lda_pop = coherence_model_lda_pop.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2bZJY1KQ_31"
   },
   "source": [
    "### Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkBGm5nPQ_31"
   },
   "outputs": [],
   "source": [
    "processed_docs_rock = df_rock['word_tokens4']\n",
    "\n",
    "# Topic model with gensim\n",
    "\n",
    "dictionary_rock = gensim.corpora.Dictionary(processed_docs_rock)\n",
    "dictionary_rock.filter_extremes(no_below=15, no_above=0.5, keep_n = 10000)\n",
    "bow_corpus_rock = [dictionary_rock.doc2bow(doc) for doc in processed_docs_rock]\n",
    "n_cores = 12\n",
    "k = 2\n",
    "lda_model_rock = gensim.models.LdaMulticore(bow_corpus_rock, num_topics=k, id2word=dictionary_rock, passes=2, random_state = 42)\n",
    "\n",
    "#lda_model_rock.save('rock.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEsxF9OrQ_32",
    "outputId": "7236fc6a-422a-4ce5-d152-1802a8457a0b"
   },
   "outputs": [],
   "source": [
    "topics_rock = lda_model_rock.print_topics(num_words=10)\n",
    "for topic in topics_rock:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztUqRwtrQ_33",
    "outputId": "8a27f42d-d5e9-416d-d084-b6e1d4e53449"
   },
   "outputs": [],
   "source": [
    "# Coherence Score\n",
    "coherence_model_lda_rock = CoherenceModel(model=lda_model_rock, texts=processed_docs_rock, dictionary=dictionary_rock, coherence='c_v')\n",
    "coherence_lda_rock = coherence_model_lda_rock.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_rock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1zEQ7OyQ_34"
   },
   "source": [
    "### Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6kn9acmQ_34"
   },
   "outputs": [],
   "source": [
    "processed_docs_country = df_country['word_tokens4']\n",
    "\n",
    "# Topic model with gensim\n",
    "\n",
    "dictionary_country = gensim.corpora.Dictionary(processed_docs_country)\n",
    "dictionary_country.filter_extremes(no_below=15, no_above=0.5, keep_n = 10000)\n",
    "bow_corpus_country = [dictionary_country.doc2bow(doc) for doc in processed_docs_country]\n",
    "n_cores = 12\n",
    "k = 2\n",
    "lda_model_country = gensim.models.LdaMulticore(bow_corpus_country, num_topics=k, id2word=dictionary_country, passes=2, random_state = 42)\n",
    "\n",
    "#lda_model_country.save('rock.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBdrePveQ_34",
    "outputId": "a0f75819-4ff6-455e-e316-25e81e5048e8"
   },
   "outputs": [],
   "source": [
    "topics_country = lda_model_country.print_topics(num_words=10)\n",
    "for topic in topics_country:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9yU4kDEgQ_35",
    "outputId": "7babad18-f06d-4f1e-f9a5-ff7907433373"
   },
   "outputs": [],
   "source": [
    "# Coherence Score\n",
    "coherence_model_lda_country = CoherenceModel(model=lda_model_country, texts=processed_docs_country, dictionary=dictionary_country, coherence='c_v')\n",
    "coherence_lda_country = coherence_model_lda_country.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpz-ODpUQ_36"
   },
   "source": [
    "### Second topic modelling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xeqllr-YQ_37"
   },
   "outputs": [],
   "source": [
    "X = df_select[['word_tokens4', 'tag']]\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LfHgIdu9Q_37",
    "outputId": "4da25c8a-029d-4752-c85b-80b136737ce0"
   },
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "\n",
    "X_test_rap = X_test.word_tokens4[X_test['tag']=='rap']\n",
    "X_test_pop = X_test.word_tokens4[X_test['tag']=='pop']\n",
    "X_test_rock = X_test.word_tokens4[X_test['tag']=='rock']\n",
    "X_test_country = X_test.word_tokens4[X_test['tag']=='country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMmcKu4EQ_38"
   },
   "outputs": [],
   "source": [
    "processed_docs = X_train['word_tokens4']\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n = 10000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "#n_cores = 12\n",
    "k = 2\n",
    "lda_model = gensim.models.LdaModel(bow_corpus, num_topics=k, id2word=dictionary, passes=10, random_state=42)\n",
    "\n",
    "#lda_model.save('train.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88v19-g6Q_39",
    "outputId": "d1aad504-baf1-4568-fa20-f34eb084cddb"
   },
   "outputs": [],
   "source": [
    "topics = lda_model.print_topics(num_words=20)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XveYXPvUQ_3-"
   },
   "outputs": [],
   "source": [
    "rap = []\n",
    "for doc in X_test_rap:\n",
    "    new_doc_bow = dictionary.doc2bow(doc)\n",
    "    rap.append(lda_model.get_document_topics(new_doc_bow))\n",
    "\n",
    "rock = []\n",
    "for doc in X_test_rock:\n",
    "    new_doc_bow = dictionary.doc2bow(doc)\n",
    "    rock.append(lda_model.get_document_topics(new_doc_bow))\n",
    "\n",
    "pop = []\n",
    "for doc in X_test_pop:\n",
    "    new_doc_bow = dictionary.doc2bow(doc)\n",
    "    pop.append(lda_model.get_document_topics(new_doc_bow))\n",
    "\n",
    "country = []\n",
    "for doc in X_test_country:\n",
    "    new_doc_bow = dictionary.doc2bow(doc)\n",
    "    country.append(lda_model.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-h-nklWiQ_3-"
   },
   "source": [
    "#### Rap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fb01c5y-Q_3-",
    "outputId": "d2a4104d-0eb4-4bc5-ffdb-d2c27360dbf6"
   },
   "outputs": [],
   "source": [
    "topic0 = []\n",
    "topic1 = []\n",
    "\n",
    "for song in rap:\n",
    "    for c in song:\n",
    "        if c[0] == 0:\n",
    "            topic0.append(c[1])\n",
    "        elif c[0] == 1:\n",
    "            topic1.append(c[1])\n",
    "\n",
    "    \n",
    "\n",
    "avg_topic0 = sum(topic0)/len(topic0)\n",
    "avg_topic1 = sum(topic1)/len(topic1)\n",
    "\n",
    "print(avg_topic0)\n",
    "print(avg_topic1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvyICNZUQ_3_",
    "outputId": "0d75498b-e7c5-42df-ab24-a6b692074f55"
   },
   "outputs": [],
   "source": [
    "# font settings\n",
    "font = {'family' : 'arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# figure settings\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# data for the boxplot\n",
    "data_box = [topic0,topic1]\n",
    "\n",
    "# removes frame\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xticklabels(['Topic 0', 'Topic 1'])\n",
    "\n",
    "# enables adding colors\n",
    "bp = ax.boxplot(data_box, widths=0.5, patch_artist=True)\n",
    "\n",
    "# change outline color, fill color and linewidth of the boxes\n",
    "\n",
    "# looping over all boxes to change the line color and fill\n",
    "for box in bp['boxes']:\n",
    "    box.set( color='#3d708f', linewidth=2)\n",
    "    box.set( facecolor = '#3d708f' )\n",
    "\n",
    "# lopping over all whiskers to change color\n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set(color='#004c6d', linewidth=2)\n",
    "\n",
    "# lopping over all caps to change color\n",
    "for cap in bp['caps']:\n",
    "    cap.set(color='#004c6d', linewidth=2)\n",
    "\n",
    "# lopping over all medians to change color\n",
    "for median in bp['medians']:\n",
    "    median.set(color='#94bed9', linewidth=2)\n",
    "\n",
    "# lopping over all outlier marks to change color and apperance\n",
    "for flier in bp['fliers']:\n",
    "    flier.set(marker='o', color='#004c6d', alpha=0.2)\n",
    "\n",
    "# setting xticklabels\n",
    "ax.set_xticklabels(['Topic 0', 'Topic 1'])\n",
    "\n",
    "# setting title\n",
    "ax.set_title('Rap', pad = 20)\n",
    "\n",
    "# removing y-ticks and x-ticks\n",
    "ax.tick_params(axis='both', which='both', length=0) \n",
    "\n",
    "# saving figure\n",
    "fig.savefig('box0.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHzy-lFzQ_4A"
   },
   "source": [
    "#### Pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_YYNgJpQ_4A",
    "outputId": "6730611d-bd0c-4f74-8487-4c5be0dde914"
   },
   "outputs": [],
   "source": [
    "topic0 = []\n",
    "topic1 = []\n",
    "\n",
    "for song in pop:\n",
    "    for c in song:\n",
    "        if c[0] == 0:\n",
    "            topic0.append(c[1])\n",
    "        elif c[0] == 1:\n",
    "            topic1.append(c[1])\n",
    "\n",
    "    \n",
    "\n",
    "avg_topic0 = sum(topic0)/len(topic0)\n",
    "avg_topic1 = sum(topic1)/len(topic1)\n",
    "\n",
    "print(avg_topic0)\n",
    "print(avg_topic1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITayyOSQQ_4B",
    "outputId": "3d01bc25-62b0-4214-8bf7-da708391320b"
   },
   "outputs": [],
   "source": [
    "# font settings\n",
    "font = {'family' : 'arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# figure settings\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# data for the boxplot\n",
    "data_box = [topic0,topic1]\n",
    "\n",
    "# removes frame\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xticklabels(['Topic 0', 'Topic 1'])\n",
    "\n",
    "# enables adding colors\n",
    "bp = ax.boxplot(data_box, widths=0.5, patch_artist=True)\n",
    "\n",
    "# change outline color, fill color and linewidth of the boxes\n",
    "\n",
    "# looping over all boxes to change the line color and fill\n",
    "for box in bp['boxes']:\n",
    "    box.set( color='#3d708f', linewidth=2)\n",
    "    box.set( facecolor = '#3d708f' )\n",
    "\n",
    "# lopping over all whiskers to change color\n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set(color='#004c6d', linewidth=2)\n",
    "\n",
    "# lopping over all caps to change color\n",
    "for cap in bp['caps']:\n",
    "    cap.set(color='#004c6d', linewidth=2)\n",
    "\n",
    "# lopping over all medians to change color\n",
    "for median in bp['medians']:\n",
    "    median.set(color='#94bed9', linewidth=2)\n",
    "\n",
    "# lopping over all outlier marks to change color and apperance\n",
    "for flier in bp['fliers']:\n",
    "    flier.set(marker='o', color='#004c6d', alpha=0.2)\n",
    "\n",
    "# setting xticklabels\n",
    "ax.set_xticklabels(['Topic 0', 'Topic 1'])\n",
    "\n",
    "# setting title\n",
    "ax.set_title('Pop', pad = 20)\n",
    "\n",
    "# removing y-ticks and x-ticks\n",
    "ax.tick_params(axis='both', which='both', length=0) \n",
    "\n",
    "# saving figure\n",
    "fig.savefig('box1.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noLaUrjzQ_4C"
   },
   "source": [
    "#### Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AG2Htb1-Q_4D",
    "outputId": "8a810340-9c80-41c8-c1eb-0d9c8ae0f229"
   },
   "outputs": [],
   "source": [
    "topic0 = []\n",
    "topic1 = []\n",
    "\n",
    "for song in rock:\n",
    "    for c in song:\n",
    "        if c[0] == 0:\n",
    "            topic0.append(c[1])\n",
    "        elif c[0] == 1:\n",
    "            topic1.append(c[1])\n",
    "\n",
    "    \n",
    "\n",
    "avg_topic0 = sum(topic0)/len(topic0)\n",
    "avg_topic1 = sum(topic1)/len(topic1)\n",
    "\n",
    "print(avg_topic0)\n",
    "print(avg_topic1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-rt8oaWQ_4D",
    "outputId": "6a12ad33-f2ab-4a49-8f94-fc24cb4d8f92"
   },
   "outputs": [],
   "source": [
    "# font settings\n",
    "font = {'family' : 'arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# figure settings\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# data for the boxplot\n",
    "data_box = [topic0,topic1]\n",
    "\n",
    "# removes frame\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xticklabels(['Topic 0', 'Topic 1'])\n",
    "\n",
    "# enables adding colors\n",
    "bp = ax.boxplot(data_box, widths=0.5, patch_artist=True)\n",
    "\n",
    "# change outline color, fill color and linewidth of the boxes\n",
    "\n",
    "# looping over all boxes to change the line color and fill\n",
    "for box in bp['boxes']:\n",
    "    box.set( color='#3d708f', linewidth=2)\n",
    "    box.set( facecolor = '#3d708f' )\n",
    "\n",
    "# lopping over all whiskers to change color\n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set(color='#004c6d', linewidth=2)\n",
    "\n",
    "# lopping over all caps to change color\n",
    "for cap in bp['caps']:\n",
    "    cap.set(color='#004c6d', linewidth=2)\n",
    "\n",
    "# lopping over all medians to change color\n",
    "for median in bp['medians']:\n",
    "    median.set(color='#94bed9', linewidth=2)\n",
    "\n",
    "# lopping over all outlier marks to change color and apperance\n",
    "for flier in bp['fliers']:\n",
    "    flier.set(marker='o', color='#004c6d', alpha=0.2)\n",
    "\n",
    "# setting xticklabels\n",
    "ax.set_xticklabels(['Topic 0', 'Topic 1'])\n",
    "\n",
    "# setting title\n",
    "ax.set_title('Rock', pad = 20)\n",
    "\n",
    "# removing y-ticks and x-ticks\n",
    "ax.tick_params(axis='both', which='both', length=0) \n",
    "\n",
    "# saving figure\n",
    "fig.savefig('box2.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYlTz3D6Q_4E"
   },
   "source": [
    "#### Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7z3UpLmFQ_4E",
    "outputId": "be986526-43bd-4697-bd84-47684cfd2909"
   },
   "outputs": [],
   "source": [
    "topic0 = []\n",
    "topic1 = []\n",
    "\n",
    "for song in country:\n",
    "    for c in song:\n",
    "        if c[0] == 0:\n",
    "            topic0.append(c[1])\n",
    "        elif c[0] == 1:\n",
    "            topic1.append(c[1])\n",
    "\n",
    "    \n",
    "\n",
    "avg_topic0 = sum(topic0)/len(topic0)\n",
    "avg_topic1 = sum(topic1)/len(topic1)\n",
    "\n",
    "print(avg_topic0)\n",
    "print(avg_topic1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqx7WuPmQ_4F",
    "outputId": "b0e195f1-54fe-4410-fbbe-05dcdbdfa40b"
   },
   "outputs": [],
   "source": [
    "# font settings\n",
    "font = {'family' : 'arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# figure settings\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# data for the boxplot\n",
    "data_box = [topic0,topic1]\n",
    "\n",
    "# removes frame\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xticklabels(['Topic 0', 'Topic 1'])\n",
    "\n",
    "# enables adding colors\n",
    "bp = ax.boxplot(data_box, widths=0.5, patch_artist=True)\n",
    "\n",
    "# change outline color, fill color and linewidth of the boxes\n",
    "\n",
    "# looping over all boxes to change the line color and fill\n",
    "for box in bp['boxes']:\n",
    "    box.set( color='#3d708f', linewidth=2)\n",
    "    box.set( facecolor = '#3d708f' )\n",
    "\n",
    "# lopping over all whiskers to change color\n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set(color='#004c6d', linewidth=2)\n",
    "\n",
    "# lopping over all caps to change color\n",
    "for cap in bp['caps']:\n",
    "    cap.set(color='#004c6d', linewidth=2)\n",
    "\n",
    "# lopping over all medians to change color\n",
    "for median in bp['medians']:\n",
    "    median.set(color='#94bed9', linewidth=2)\n",
    "\n",
    "# lopping over all outlier marks to change color and apperance\n",
    "for flier in bp['fliers']:\n",
    "    flier.set(marker='o', color='#004c6d', alpha=0.2)\n",
    "\n",
    "# setting xticklabels\n",
    "ax.set_xticklabels(['Topic 0', 'Topic 1'])\n",
    "\n",
    "# setting title\n",
    "ax.set_title('Country', pad = 20)\n",
    "\n",
    "# removing y-ticks and x-ticks\n",
    "ax.tick_params(axis='both', which='both', length=0) \n",
    "\n",
    "# saving figure\n",
    "fig.savefig('box3.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RD5FByGAQ_4G"
   },
   "source": [
    "## Topic modelling with four topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1EHo0cPQ_4H"
   },
   "outputs": [],
   "source": [
    "processed_docs = X_train['word_tokens4']\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n = 10000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "#n_cores = 12\n",
    "k = 4\n",
    "lda_model = gensim.models.LdaModel(bow_corpus, num_topics=k, id2word=dictionary, passes=10, random_state=42)\n",
    "\n",
    "#lda_model.save('train.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zqs_XBZ_Q_4H",
    "outputId": "e15fa96e-79ad-4d76-e3cc-636777374373"
   },
   "outputs": [],
   "source": [
    "topics = lda_model.print_topics(num_words=20)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrbwKBwuQ_4I"
   },
   "outputs": [],
   "source": [
    "rap = []\n",
    "for doc in X_test_rap:\n",
    "    new_doc_bow = dictionary.doc2bow(doc)\n",
    "    rap.append(lda_model.get_document_topics(new_doc_bow))\n",
    "\n",
    "rock = []\n",
    "for doc in X_test_rock:\n",
    "    new_doc_bow = dictionary.doc2bow(doc)\n",
    "    rock.append(lda_model.get_document_topics(new_doc_bow))\n",
    "\n",
    "pop = []\n",
    "for doc in X_test_pop:\n",
    "    new_doc_bow = dictionary.doc2bow(doc)\n",
    "    pop.append(lda_model.get_document_topics(new_doc_bow))\n",
    "\n",
    "country = []\n",
    "for doc in X_test_country:\n",
    "    new_doc_bow = dictionary.doc2bow(doc)\n",
    "    country.append(lda_model.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uy6UIRLHQ_4J"
   },
   "outputs": [],
   "source": [
    "avg_topic0 = []\n",
    "avg_topic1 = []\n",
    "avg_topic2 = []\n",
    "avg_topic3 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tLVo-sIQ_4J"
   },
   "source": [
    "### Rap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vc1K6__3Q_4K",
    "outputId": "619ace1b-1294-4400-9902-44b0af011c83"
   },
   "outputs": [],
   "source": [
    "topic0 = []\n",
    "topic1 = []\n",
    "topic2 = []\n",
    "topic3 = []\n",
    "\n",
    "\n",
    "for song in rap:\n",
    "    for c in song:\n",
    "        if c[0] == 0:\n",
    "            topic0.append(c[1])\n",
    "        elif c[0] == 1:\n",
    "            topic1.append(c[1])\n",
    "        elif c[0] == 2:\n",
    "            topic2.append(c[1])\n",
    "        elif c[0] == 3:\n",
    "            topic3.append(c[1])\n",
    "\n",
    "    \n",
    "avg_topic0.append(sum(topic0)/len(topic0))\n",
    "avg_topic1.append(sum(topic1)/len(topic1))\n",
    "avg_topic2.append(sum(topic2)/len(topic2))\n",
    "avg_topic3.append(sum(topic3)/len(topic3))\n",
    "\n",
    "print(avg_topic0)\n",
    "print(avg_topic1)\n",
    "print(avg_topic2)\n",
    "print(avg_topic3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHg-w56DQ_4L"
   },
   "source": [
    "### Pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNC14xoDQ_4L",
    "outputId": "d68aea59-adaf-4406-da56-8bd7ade08e57"
   },
   "outputs": [],
   "source": [
    "topic0 = []\n",
    "topic1 = []\n",
    "topic2 = []\n",
    "topic3 = []\n",
    "\n",
    "\n",
    "for song in pop:\n",
    "    for c in song:\n",
    "        if c[0] == 0:\n",
    "            topic0.append(c[1])\n",
    "        elif c[0] == 1:\n",
    "            topic1.append(c[1])\n",
    "        elif c[0] == 2:\n",
    "            topic2.append(c[1])\n",
    "        elif c[0] == 3:\n",
    "            topic3.append(c[1])\n",
    "\n",
    "    \n",
    "avg_topic0.append(sum(topic0)/len(topic0))\n",
    "avg_topic1.append(sum(topic1)/len(topic1))\n",
    "avg_topic2.append(sum(topic2)/len(topic2))\n",
    "avg_topic3.append(sum(topic3)/len(topic3))\n",
    "\n",
    "print(avg_topic0)\n",
    "print(avg_topic1)\n",
    "print(avg_topic2)\n",
    "print(avg_topic3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-f5R8hpQ_4M"
   },
   "source": [
    "### Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUyrwf4eQ_4M",
    "outputId": "32a849ae-7281-41d3-894e-aa1d6bb7c06a"
   },
   "outputs": [],
   "source": [
    "topic0 = []\n",
    "topic1 = []\n",
    "topic2 = []\n",
    "topic3 = []\n",
    "\n",
    "\n",
    "for song in rock:\n",
    "    for c in song:\n",
    "        if c[0] == 0:\n",
    "            topic0.append(c[1])\n",
    "        elif c[0] == 1:\n",
    "            topic1.append(c[1])\n",
    "        elif c[0] == 2:\n",
    "            topic2.append(c[1])\n",
    "        elif c[0] == 3:\n",
    "            topic3.append(c[1])\n",
    "\n",
    "    \n",
    "avg_topic0.append(sum(topic0)/len(topic0))\n",
    "avg_topic1.append(sum(topic1)/len(topic1))\n",
    "avg_topic2.append(sum(topic2)/len(topic2))\n",
    "avg_topic3.append(sum(topic3)/len(topic3))\n",
    "\n",
    "print(avg_topic0)\n",
    "print(avg_topic1)\n",
    "print(avg_topic2)\n",
    "print(avg_topic3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpnCiARPQ_4N"
   },
   "source": [
    "### Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gr3YcQMwQ_4N",
    "outputId": "2fd8e3eb-92a1-49a1-ef53-9ca27326e16a"
   },
   "outputs": [],
   "source": [
    "topic0 = []\n",
    "topic1 = []\n",
    "topic2 = []\n",
    "topic3 = []\n",
    "\n",
    "\n",
    "for song in country:\n",
    "    for c in song:\n",
    "        if c[0] == 0:\n",
    "            topic0.append(c[1])\n",
    "        elif c[0] == 1:\n",
    "            topic1.append(c[1])\n",
    "        elif c[0] == 2:\n",
    "            topic2.append(c[1])\n",
    "        elif c[0] == 3:\n",
    "            topic3.append(c[1])\n",
    "\n",
    "    \n",
    "avg_topic0.append(sum(topic0)/len(topic0))\n",
    "avg_topic1.append(sum(topic1)/len(topic1))\n",
    "avg_topic2.append(sum(topic2)/len(topic2))\n",
    "avg_topic3.append(sum(topic3)/len(topic3))\n",
    "\n",
    "print(avg_topic0)\n",
    "print(avg_topic1)\n",
    "print(avg_topic2)\n",
    "print(avg_topic3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIewnnf3Q_4N"
   },
   "outputs": [],
   "source": [
    "top = pd.DataFrame([avg_topic0,avg_topic1,avg_topic2,avg_topic3])\n",
    "cols = ['Rap','Pop','Rock','Country']\n",
    "top.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAOtKN8hQ_4O",
    "outputId": "d45ba018-6743-41db-e6fe-8e0329028d6c"
   },
   "outputs": [],
   "source": [
    "# font settings\n",
    "font = {'family' : 'arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "\n",
    "#fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (10,5)) # canvas\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "# removes frame\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "# setting title\n",
    "ax.set_title('Topics', pad = 20) \n",
    "\n",
    "# removing y-ticks and x-ticks\n",
    "ax.tick_params(axis='both', which='both', length=0) \n",
    "\n",
    "# plot adjustments\n",
    "top.plot(kind='bar', ax = ax)\n",
    "ax.set_title('Topics', pad = 20) # setting title\n",
    "ax.set_xlabel(xlabel='')\n",
    "ax.set_xticklabels(['Topic 0', 'Topic 1', 'Topic 2', 'Topic 3'], rotation = 0)\n",
    "ax.get_children()[0].set_color('#004c6d')\n",
    "ax.get_children()[1].set_color('#004c6d')\n",
    "ax.get_children()[2].set_color('#004c6d')\n",
    "ax.get_children()[3].set_color('#004c6d')\n",
    "ax.get_children()[4].set_color('#3d708f')\n",
    "ax.get_children()[5].set_color('#3d708f')\n",
    "ax.get_children()[6].set_color('#3d708f')\n",
    "ax.get_children()[7].set_color('#3d708f')\n",
    "ax.get_children()[8].set_color('#6996b3')\n",
    "ax.get_children()[9].set_color('#6996b3')\n",
    "ax.get_children()[10].set_color('#6996b3')\n",
    "ax.get_children()[11].set_color('#6996b3')\n",
    "ax.get_children()[12].set_color('#94bed9')\n",
    "ax.get_children()[13].set_color('#94bed9')\n",
    "ax.get_children()[14].set_color('#94bed9')\n",
    "ax.get_children()[15].set_color('#94bed9')\n",
    "ax.legend(['Rap', 'Pop', 'Rock', 'Country'], frameon=False, loc='best')\n",
    "\n",
    "# saving figure\n",
    "fig.savefig('4topics.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32YDSa_nQ_3i"
   },
   "source": [
    "## Sentiment analysis using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMEV81aIQ_3j"
   },
   "outputs": [],
   "source": [
    "# analyzer function\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def tag(i):\n",
    "    return analyzer.polarity_scores(i)\n",
    "\n",
    "p = Pool(12)\n",
    "\n",
    "inputs = df_select['lyrics_clean']\n",
    "outputs = []\n",
    "\n",
    "for result in p.imap(tag, inputs):\n",
    "    outputs.append(result)\n",
    "    \n",
    "p.close()\n",
    "\n",
    "# setting the results on the df_select dataframe\n",
    "df_select['sentiment'] = [outputs[i] for i in range(len(df_select))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNuZXN6vQ_3j"
   },
   "source": [
    "Splitting the sentiment dictionary into four columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ce5f4ceqQ_3k"
   },
   "outputs": [],
   "source": [
    "df_select['neg'] = df_select.sentiment.apply(lambda i: [k for j, k in i.items() if j == 'neg'][0])\n",
    "df_select['neu'] = df_select.sentiment.apply(lambda i: [k for j, k in i.items() if j == 'neu'][0])\n",
    "df_select['pos'] = df_select.sentiment.apply(lambda i: [k for j, k in i.items() if j == 'pos'][0])\n",
    "df_select['compound'] = df_select.sentiment.apply(lambda i: [k for j, k in i.items() if j == 'compound'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "swqSvSCVQ_3k"
   },
   "source": [
    "Plot for the sentiment composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AilDQ6vuQ_3l",
    "outputId": "f4dc00d0-686e-487a-979a-2ea3727c4e9f"
   },
   "outputs": [],
   "source": [
    "data_sent = pd.DataFrame(df_select.groupby('tag').mean()[['neg','pos']])\n",
    "\n",
    "# font settings\n",
    "font = {'family' : 'arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "\n",
    "#fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (10,5)) # canvas\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "# removes frame\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "# setting title\n",
    "ax.set_title('Characters per word', pad = 20) \n",
    "\n",
    "# removing y-ticks and x-ticks\n",
    "ax.tick_params(axis='both', which='both', length=0) \n",
    "\n",
    "# plot adjustments\n",
    "data_sent.plot(kind='bar', stacked = True, ax = ax)\n",
    "ax.set_title('Sentiment composition', pad = 20) # setting title\n",
    "ax.set_xlabel(xlabel='')\n",
    "ax.set_ylim(top=0.35)\n",
    "ax.set_xticklabels(['Country', 'Pop', 'Rap', 'Rock'], rotation = 0)\n",
    "ax.get_children()[0].set_color('#004c6d')\n",
    "ax.get_children()[1].set_color('#004c6d')\n",
    "ax.get_children()[2].set_color('#004c6d')\n",
    "ax.get_children()[3].set_color('#004c6d')\n",
    "ax.get_children()[4].set_color('#94bed9')\n",
    "ax.get_children()[5].set_color('#94bed9')\n",
    "ax.get_children()[6].set_color('#94bed9')\n",
    "ax.get_children()[7].set_color('#94bed9')\n",
    "ax.get_children()[8].set_color('#6996b3')\n",
    "ax.get_children()[9].set_color('#6996b3')\n",
    "ax.get_children()[10].set_color('#6996b3')\n",
    "ax.get_children()[11].set_color('#6996b3')\n",
    "ax.legend(['Negative','Positive'], frameon=False)\n",
    "\n",
    "# saving figure\n",
    "fig.savefig('sentiment.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UT3buHI4Q_3m",
    "outputId": "c171337e-6bda-491b-a5f6-3ce473af4792"
   },
   "outputs": [],
   "source": [
    "df_select.groupby('tag')['compound'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KnsgZSttQ_4P"
   },
   "source": [
    "## Prediction - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zCsB1vVPQ_4P"
   },
   "source": [
    "### Baseline model - only BoW + no grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPvHmJKrQ_4P"
   },
   "outputs": [],
   "source": [
    "# selecting features\n",
    "X = df_select[['lyrics_clean','unique_words','word_count','neg', 'pos']]\n",
    "\n",
    "# target variable\n",
    "y = df_select['tag']\n",
    "\n",
    "# splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    #(['unique_words','word_count'], None),\n",
    "    ('lyrics_clean',CountVectorizer())\n",
    "])\n",
    "count_train= mapper.fit_transform(X_train)\n",
    "count_test = mapper.transform(X_test)\n",
    "\n",
    "# classifier and fitting\n",
    "LogReg = LogisticRegression()\n",
    "LogReg.fit(count_train, y_train)\n",
    "\n",
    "# prediction\n",
    "pred = LogReg.predict(count_test)\n",
    "\n",
    "print('Accuracy score of:' + str(metrics.accuracy_score(y_test,pred)))\n",
    "print(metrics.confusion_matrix(y_test,pred, labels = ['pop','rock','rap','country']))\n",
    "print(classification_report(y_test, pred, labels = ['pop','rock','rap','country']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_O9QwRvJQ_4Q"
   },
   "outputs": [],
   "source": [
    "table1 = pd.DataFrame(metrics.confusion_matrix(y_test,pred, labels = ['pop','rock','rap','country']))\n",
    "cols = ['Pop','Rock','Rap','Country']\n",
    "table1.columns = cols\n",
    "table1.index = cols \n",
    "table1.to_csv('table1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8SVxcu2Q_4Q"
   },
   "source": [
    "### Only BoW + grid search on hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4Dk_rT7Q_4R"
   },
   "outputs": [],
   "source": [
    "# penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# hyperparameter space\n",
    "C = = np.logspace(0, 4, 10)\n",
    "\n",
    "# hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# 5-fold cross validation\n",
    "clf = GridSearchCV(LogReg, hyperparameters, cv=5, verbose=0)\n",
    "\n",
    "best_model = clf.fit(count_train, y_train)\n",
    "\n",
    "best_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yx5gaCzQQ_4R"
   },
   "outputs": [],
   "source": [
    "# selecting features\n",
    "X = df_select[['lyrics_clean','unique_words','word_count','neg', 'pos']]\n",
    "\n",
    "# target variable\n",
    "y = df_select['tag']\n",
    "\n",
    "# splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "mapper = DataFrameMapper([('lyrics_clean',CountVectorizer())])\n",
    "count_train= mapper.fit_transform(X_train)\n",
    "count_test = mapper.transform(X_test)\n",
    "\n",
    "# classifier and fitting\n",
    "LogReg = LogisticRegression(C = 1.0, penalty = 'l1')\n",
    "LogReg.fit(count_train, y_train)\n",
    "\n",
    "# prediction\n",
    "pred = LogReg.predict(count_test)\n",
    "\n",
    "print('Accuracy score of:' + str(metrics.accuracy_score(y_test,pred)))\n",
    "print(metrics.confusion_matrix(y_test,pred, labels = ['pop','rock','rap','country']))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUa90vH2Q_4S"
   },
   "outputs": [],
   "source": [
    "table1 = pd.DataFrame(metrics.confusion_matrix(y_test,pred, labels = ['pop','rock','rap','country']))\n",
    "cols = ['Pop','Rock','Rap','Country']\n",
    "table1.columns = cols\n",
    "table1.index = cols \n",
    "table1.to_csv('table2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3g3nf4jxQ_4S"
   },
   "source": [
    "### BoW + additional features and grid search on hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7lmkt2wQ_4S"
   },
   "outputs": [],
   "source": [
    "# selecting features\n",
    "X = df_select[['lyrics_clean','unique_words','word_count','neg', 'pos']]\n",
    "\n",
    "# target variable\n",
    "y = df_select['tag']\n",
    "\n",
    "# splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    (['unique_words','word_count','neg', 'pos'], None),\n",
    "    ('lyrics_clean',CountVectorizer())\n",
    "])\n",
    "count_train= mapper.fit_transform(X_train)\n",
    "count_test = mapper.transform(X_test)\n",
    "\n",
    "\n",
    "# classifier and fitting\n",
    "LogReg = LogisticRegression(C = 1.0, penalty = 'l1')\n",
    "LogReg.fit(count_train, y_train)\n",
    "\n",
    "# prediction\n",
    "pred = LogReg.predict(count_test)\n",
    "\n",
    "print('Accuracy score of:' + str(metrics.accuracy_score(y_test,pred)))\n",
    "print(metrics.confusion_matrix(y_test,pred, labels = ['pop','rock','rap','country']))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tyHz13JQ_4T"
   },
   "outputs": [],
   "source": [
    "# penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# 5-fold cross validation\n",
    "clf = GridSearchCV(LogReg, hyperparameters, cv=5, verbose=0)\n",
    "\n",
    "best_model = clf.fit(count_train, y_train)\n",
    "\n",
    "best_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKawDk7cQ_4U"
   },
   "outputs": [],
   "source": [
    "table1 = pd.DataFrame(metrics.confusion_matrix(y_test,pred, labels = ['pop','rock','rap','country']))\n",
    "cols = ['Pop','Rock','Rap','Country']\n",
    "table1.columns = cols\n",
    "table1.index = cols \n",
    "table1.to_csv('table3.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EjCBiKnHQ_3A",
    "PdzzlL1_Q_3s",
    "EW2kaM3TQ_3t"
   ],
   "name": "Final-documentation.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
